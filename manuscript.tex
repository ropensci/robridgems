\documentclass[author-year, review, 11pt]{components/elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}
\usepackage{lineno} % add
  \linenumbers % turns line numbering on
\bibliographystyle{elsarticle-harv}
\biboptions{sort&compress} % For natbib
\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{\it \hfill\today}%
 \let\@evenfoot\@oddfoot}
\makeatother

% to avoid tightlist pandoc error
\def\tightlist{}

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Building bridges between data providers and users: best practices and lessons learned},
            colorlinks=true,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header



\begin{document}
\begin{frontmatter}

  \title{Building bridges between data providers and users: best practices and
lessons learned}
    \author[cstar]{foo bar\corref{c1}}
   \ead{stuff(at)ropensci.org} 
   \cortext[c1]{Corresponding author}
      \address[cstar]{rOpenSci, Museum of Paleontology, University of California, Berkeley,
CA, USA}
  
  \begin{abstract}
  Corresponding Author:
  
  foo bar
  
  rOpenSci, Museum of Paleontology, University of California, Berkeley,
  CA, USA
  
  Email address:
  \href{mailto:stuff@ropensci.org}{\nolinkurl{stuff@ropensci.org}}
  
  \newpage
  
  abstract text \ldots{}
  \end{abstract}
  
 \end{frontmatter}


\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

intro text \ldots{}

\hypertarget{outline-from-chat-btw-carlscott}{%
\section{OUTLINE (from chat btw
Carl/Scott)}\label{outline-from-chat-btw-carlscott}}

\begin{itemize}
\tightlist
\item
  use cases necessitate FTP vs.~csv dump vs.~rest API vs etc.
\item
  within APIS: what are best practices
\item
  discoverability - how do you find out if an API has data (taxonomic,
  geospatial, etc.) you want?
\item
  data formats: tabular vs.~JSON/XML, etc.
\item
  combining data: e.g., using identifiers instead of names
\item
  best practices in relational databases, briefly
\item
  ropensci filling gaps btwn data providers and scientists

  \begin{itemize}
  \tightlist
  \item
    communicating from data provdiers to sci.
  \item
    communicating from sci. to data provdiers
  \end{itemize}
\end{itemize}

\hypertarget{overview-of-the-landscape}{%
\section{Overview of the landscape}\label{overview-of-the-landscape}}

There is an increasing amount of data available to researchers.
Leveraging that data efficiently and reproducibly ideally requires
software. However, researchers have limited time - thus, most rightly
focus on the science. Furthermore, we want researchers to focus on
science. Given this, there are a number of issues that others must
handle:

\begin{itemize}
\tightlist
\item
  We need well made open source software to help researchers leverage
  data
\item
  We need people as bridges between data providers and data users
  (researchers)
\item
  Slack needs to be tacken up to help small scientific databases
\item
  We need best practices for working with data
\end{itemize}

\hypertarget{matching-the-data-format-to-the-use-case}{%
\section{Matching the data format to the use
case}\label{matching-the-data-format-to-the-use-case}}

There are innumerable data formats, and there is no one data format that
is best for every situation. Ideally, one leverages the right data
format for their use case. The following use cases highlight some of the
diversity and what data formats they best match.

\begin{itemize}
\tightlist
\item
  Large amount of data: This varies surely, and depends on connection
  speeds, computers available, etc. - but for sake of argument lets say
  that \textgreater{} 1 GB is large data for this use case. It doesn't
  make a lot of sense to provide this through an API, and makes sense to
  provide as a compressed format. Data of this type makes sense to
  provide via FTP or similar (Amazon S3, http file server, etc.).
\item
  Small amount of data: Probably the majority of data use cases are
  ``small data''. For example, a spreadsheet with 100 or 100,000 rows is
  small data. The delivery mechanism can be more flexible with this kind
  of data. You can definitely serve this data over FTP, but can also
  simply provide csv/tsv files, and can serve the data over an API.
\item
  Data constantly changing: This is a good use case for delivering data
  via an API. APIs connect to an underlying database that can change as
  much as the data providers need. However, the API ideally changes only
  very slowly so that clients can depend on the interface. It's easier
  to update data incrementally over an API than if there's small changes
  in lots of files on an FTP server.
\end{itemize}

\hypertarget{discoverability}{%
\section{Discoverability}\label{discoverability}}

Discovering what kind of data you can get from a data source before
actually getting that data is an imporatnt one. For example, say there's
a database with 1000 GB of data. You don't want to have to download that
database to your own machine, then search through it to find the data
you want. Ideally there's a fast way to query a database (perhaps it's
metadata) before delving into the data fetching process that will likely
take longer.

Unfortunately, most datasets are very lacking in metadata. However, when
metadata is well filled out, it makes data discovery much easier. For
example, \ldots{}

Suggestions?: do x. y, and z

\hypertarget{bridges-between-data-providers-and-users}{%
\section{Bridges between data providers and
users}\label{bridges-between-data-providers-and-users}}

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

This project was supported in part by the Alfred P Sloan Foundation
(Grant No.~G-2014-13485), and in part by the Helmsley Foundation (Grant
No.~2016PG-BRI004).

\hypertarget{references}{%
\section{References}\label{references}}

\end{document}


